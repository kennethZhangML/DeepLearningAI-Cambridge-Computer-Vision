{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os \r\n",
    "import numpy as np \r\n",
    "import pandas as pd \r\n",
    "\r\n",
    "import tensorflow as tf \r\n",
    "from tensorflow import keras\r\n",
    "from tensorflow.keras import regularizers, initializers, optimizers\r\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_dir = os.path.join(\"C:/Users/kzhan/Desktop/archive\")\r\n",
    "training_dir = os.path.join(data_dir + \"/training\")\r\n",
    "testing_dir = os.path.join(data_dir + \"/testing\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batch_size, epochs = 64, 20\r\n",
    "learning_rate = 0.01\r\n",
    "\r\n",
    "# ------------------------------------------------------------------------- #\r\n",
    "# Define a training_dataset using the image_dataset_from_directory function #\r\n",
    "# ------------------------------------------------------------------------- #\r\n",
    "def get_training_dataset(training_dir):\r\n",
    "    training_ds = tf.keras.preprocessing.image_dataset_from_directory(training_dir,\r\n",
    "    batch_size = batch_size, seed = 123, subset = \"training\", color_mode = 'rgb', shuffle = True,\r\n",
    "    validation_split = 0.2)\r\n",
    "    return training_ds\r\n",
    "\r\n",
    "# --------------------------------------------------------------------------- #\r\n",
    "# Define a validation_dataset using the image_dataset_from_directory function #\r\n",
    "# --------------------------------------------------------------------------- #\r\n",
    "def get_validation_dataset(training_dir):\r\n",
    "    validation_ds = tf.keras.preprocessing.image_dataset_from_directory(training_dir,\r\n",
    "    batch_size = batch_size, seed = 123, subset = \"validation\", validation_split = 0.2,\r\n",
    "    color_mode = 'rgb', shuffle = False)\r\n",
    "    return validation_ds\r\n",
    "\r\n",
    "# *Optional*\r\n",
    "# Define a testing_dataset #\r\n",
    "def get_testing_dataset(testing_dir):\r\n",
    "    testing_ds = tf.keras.preprocessing.image_dataset_from_directory(testing_dir,\r\n",
    "    batch_size = batch_size, shuffle = False, color_mode='rgb')\r\n",
    "\r\n",
    "training_ds = get_training_dataset(training_dir)\r\n",
    "validation_ds = get_validation_dataset(training_dir)\r\n",
    "testing_ds = get_testing_dataset(testing_dir)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 2870 files belonging to 4 classes.\n",
      "Using 2296 files for training.\n",
      "Found 2870 files belonging to 4 classes.\n",
      "Using 574 files for validation.\n",
      "Found 394 files belonging to 4 classes.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ------------------- #\r\n",
    "# Define an AUTOTUNER #\r\n",
    "# ------------------- #\r\n",
    "def autotuner():\r\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\r\n",
    "    return AUTOTUNE\r\n",
    "\r\n",
    "# ---------------------------------------------- #\r\n",
    "# Cache, Shuffle, and Prefetch the training data #\r\n",
    "# ---------------------------------------------- #\r\n",
    "def dataset_prefetch_cache_shuffle(training_data):\r\n",
    "    training_data = training_data.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)\r\n",
    "    return training_data\r\n",
    "\r\n",
    "# -------------------------------------- #\r\n",
    "# Cache and Prefetch the validation data #\r\n",
    "# -------------------------------------- #\r\n",
    "def cache_prefetch(validation_data):\r\n",
    "    validation_data = validation_data.cache().prefetch(buffer_size = AUTOTUNE)\r\n",
    "    return validation_data\r\n",
    "\r\n",
    "AUTOTUNE = autotuner()\r\n",
    "training_ds = dataset_prefetch_cache_shuffle(training_ds) #prefetch, cache, and shuffle your training data if you want\r\n",
    "validation_ds = cache_prefetch(validation_ds) # DO NOT SHUFFLE VALIDATION DATA\r\n",
    "\r\n",
    "# --------------- #\r\n",
    "# Simplified Code #\r\n",
    "# --------------- #\r\n",
    "# AUTOTUNE = tf.data.experimental.AUTOTUNE\r\n",
    "# training_ds = training_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)\r\n",
    "# validation_ds = validation_ds.cache().prefetch(buffer_size = AUTOTUNE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#similar to the first neural network we developed we must define a normalization layer\r\n",
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255) \r\n",
    "\r\n",
    "#Normalize + map our dataset using a lambda function\r\n",
    "normalized_ds = training_ds.map(lambda x_train, y_train: (normalization_layer(x_train), y_train))\r\n",
    "\r\n",
    "#Iterate through our dataset \r\n",
    "image_batch, labels_batch = next(iter(normalized_ds))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "62777786f8687b9575c0619f17c497edf6c2b58eb2992f0dd825a023781db6e8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}